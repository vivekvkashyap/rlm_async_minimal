# Quick Start Guide - Async Batch Processing

## ðŸš€ Get Started in 5 Minutes

### 1. Install Dependencies

```bash
# Activate your conda environment
conda activate tf-gpu-211

# Install required packages
cd /home/ece/Pavan/vivek_3/min_verify/rlm-minimal/
pip install -r requirements.txt
```

### 2. Set Up API Key

```bash
# Copy the example .env file
cp .env-example .env

# Edit .env and add your OpenAI API key
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

### 3. Run Examples

#### Original Example (Sequential)
```bash
python main.py
```

#### New Batch Example (Parallel) âš¡
```bash
python example_batch.py
```

#### Run Tests
```bash
python test_batch.py
```

## ðŸ“ Basic Usage

### In Your Code

```python
from rlm.rlm_repl import RLM_REPL

# Initialize RLM with batch support
rlm = RLM_REPL(
    model="gpt-5",
    recursive_model="gpt-5-nano",
    enable_logging=True,
    max_iterations=10
)

# Use it like before - batch processing happens automatically!
result = rlm.completion(
    context="Your large context here...",
    query="What information can you find?"
)

print(result)
```

### The RLM Will Automatically Use Batch Processing

When the root LLM generates code, it will now use `llm_batch()` for parallel processing:

```python
# The RLM might generate code like this:
```repl
# Split context into chunks
chunks = [context[i:i+10000] for i in range(0, len(context), 10000)]

# Process all chunks in parallel (NEW!)
prompts = [f"Find magic numbers in: {chunk}" for chunk in chunks]
results = llm_batch(prompts)

# Combine results
all_numbers = "\n".join(results)
```
```

## ðŸŽ¯ Key Concepts

### llm_query() - Sequential (Existing)
```python
# Process one at a time
result1 = llm_query("What is 2+2?")
result2 = llm_query("What is 3+3?")
result3 = llm_query("What is 4+4?")
# Total time: ~6 seconds
```

### llm_batch() - Parallel (NEW) âš¡
```python
# Process all at once
prompts = ["What is 2+2?", "What is 3+3?", "What is 4+4?"]
results = llm_batch(prompts)
# Total time: ~2 seconds (3x faster!)
```

## ðŸ”§ Configuration

### Adjust Concurrency

```python
# In REPL code generated by the LLM:
```repl
# Conservative (fewer concurrent requests)
results = llm_batch(prompts, max_concurrent=3)

# Default (balanced)
results = llm_batch(prompts, max_concurrent=10)

# Aggressive (more concurrent requests)
results = llm_batch(prompts, max_concurrent=20)
```
```

### When to Use Each

| Scenario | Use | Why |
|----------|-----|-----|
| Single query | `llm_query()` | Simple and direct |
| Sequential dependent queries | `llm_query()` | Each needs previous result |
| Multiple independent queries | `llm_batch()` | Much faster in parallel |
| Processing chunks | `llm_batch()` | Ideal for parallelization |
| Summarizing sections | `llm_batch()` | Each section independent |

## ðŸ“Š Performance Examples

### Example 1: Processing 10 Chunks

**Before (Sequential)**:
```python
results = []
for chunk in chunks:  # 10 chunks
    result = llm_query(f"Analyze: {chunk}")
    results.append(result)
# Time: 10 Ã— 2s = 20 seconds
```

**After (Parallel)**:
```python
prompts = [f"Analyze: {chunk}" for chunk in chunks]  # 10 chunks
results = llm_batch(prompts)
# Time: 1 Ã— 2s = 2 seconds (10x faster!)
```

### Example 2: Multi-Section Document

**Before (Sequential)**:
```python
summaries = []
for section in sections:  # 5 sections
    summary = llm_query(f"Summarize: {section}")
    summaries.append(summary)
# Time: 5 Ã— 2s = 10 seconds
```

**After (Parallel)**:
```python
prompts = [f"Summarize: {section}" for section in sections]  # 5 sections
summaries = llm_batch(prompts)
# Time: 1 Ã— 2s = 2 seconds (5x faster!)
```

## ðŸ› Troubleshooting

### Rate Limit Errors

**Problem**: Getting rate limit errors from API

**Solution**: Reduce max_concurrent
```python
results = llm_batch(prompts, max_concurrent=3)
```

### Timeout Errors

**Problem**: Requests timing out

**Solutions**:
1. Reduce chunk sizes
2. Reduce max_concurrent
3. Check network connection

### Memory Issues

**Problem**: Running out of memory

**Solutions**:
1. Process in smaller batches
2. Reduce max_concurrent
3. Use smaller chunks

### No Speedup

**Problem**: Parallel not faster than sequential

**Possible Causes**:
1. API rate limiting kicking in
2. Very small batch size (overhead dominates)
3. Network bottleneck

**Solutions**:
1. Check API tier and rate limits
2. Use larger batch sizes (5+ prompts)
3. Test network speed

## ðŸ“š Learn More

- **[ASYNC_BATCH_GUIDE.md](ASYNC_BATCH_GUIDE.md)**: Comprehensive documentation
- **[ARCHITECTURE.md](ARCHITECTURE.md)**: System architecture details
- **[CHANGES_SUMMARY.md](CHANGES_SUMMARY.md)**: What changed in the code

## ðŸ’¡ Tips

### 1. Let the LLM Decide
The root LLM is trained to use `llm_batch()` when appropriate. You don't need to manually specify it!

### 2. Monitor Performance
Enable logging to see execution times:
```python
rlm = RLM_REPL(enable_logging=True)
```

### 3. Start Conservative
Begin with default `max_concurrent=10`, then adjust based on results.

### 4. Chunk Smartly
- Too small: More API overhead
- Too large: Might hit token limits
- Sweet spot: 10K-100K characters per chunk

### 5. Test First
Run `test_batch.py` to verify everything works:
```bash
python test_batch.py
```

## ðŸŽ“ Example Workflow

```bash
# 1. Set up environment
conda activate tf-gpu-211
cd /home/ece/Pavan/vivek_3/min_verify/rlm-minimal/

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure API key
echo "OPENAI_API_KEY=your-key" > .env

# 4. Test the system
python test_batch.py

# 5. Run examples
python example_batch.py

# 6. Use in your code
python your_script.py
```

## âœ… Checklist

Before using in production:

- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] API key configured (`.env` file)
- [ ] Tests passing (`python test_batch.py`)
- [ ] Examples working (`python example_batch.py`)
- [ ] Logging enabled for monitoring
- [ ] Rate limits understood
- [ ] max_concurrent tuned for your use case

## ðŸ†˜ Getting Help

If you run into issues:

1. Check the error message
2. Review [ASYNC_BATCH_GUIDE.md](ASYNC_BATCH_GUIDE.md) troubleshooting section
3. Verify API key and rate limits
4. Test with smaller batches first
5. Enable logging to see what's happening

## ðŸŽ‰ Success!

You're now ready to use async batch processing with RLM!

**Key Takeaway**: The system is fully backward compatible. Your existing code works as-is, and you get automatic speedups when the LLM uses `llm_batch()` for parallel processing.

---

**Happy Hacking!** ðŸš€

