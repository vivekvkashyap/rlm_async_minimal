# Async Batch Processing Guide for RLM

## Overview

The RLM (Recursive Language Model) now supports **parallel batch processing** for sub-LLM calls, significantly improving performance when processing multiple chunks or queries.

## What Changed?

### 1. New `AsyncOpenAIClient` Class (`rlm/utils/llm.py`)

Added an async OpenAI client that supports:
- **Async completion**: Single async API call
- **Batch completion**: Multiple parallel API calls using `asyncio.gather()`
- **Synchronous wrapper**: `batch_completion_sync()` for use in sync code
- **Rate limiting**: Configurable `max_concurrent` parameter to avoid API rate limits

### 2. Enhanced `Sub_RLM` Class (`rlm/repl.py`)

Added `batch_completion()` method:
```python
def batch_completion(self, prompts: List[str], max_concurrent: int = 10) -> List[str]:
    """
    Batch LM query for parallel sub-LM calls.
    
    Args:
        prompts: List of prompts to send to the LLM
        max_concurrent: Maximum number of concurrent requests (default: 10)
        
    Returns:
        List of responses in the same order as input prompts
    """
```

### 3. New `llm_batch()` Function in REPL Environment

The REPL environment now exposes `llm_batch()` alongside `llm_query()`:

```python
# Sequential (OLD way - slow)
results = []
for chunk in chunks:
    result = llm_query(f"Analyze: {chunk}")
    results.append(result)

# Parallel (NEW way - fast!)
prompts = [f"Analyze: {chunk}" for chunk in chunks]
results = llm_batch(prompts)
```

### 4. Updated System Prompts (`rlm/utils/prompts.py`)

The system prompt now documents and encourages the use of `llm_batch()`:
- Explains the function signature
- Provides examples of converting sequential to parallel code
- Emphasizes performance benefits

## Performance Comparison

### Sequential Processing (OLD)
```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ llm_query() │───▶│ llm_query() │───▶│ llm_query() │
│   ~2 sec    │    │   ~2 sec    │    │   ~2 sec    │
└─────────────┘    └─────────────┘    └─────────────┘
                Total: ~6 seconds
```

### Parallel Processing (NEW)
```
┌─────────────┐
│ llm_batch() │───┬───▶ API Call 1 ──┐
│             │   ├───▶ API Call 2 ──┼───▶ Results
│             │   └───▶ API Call 3 ──┘
└─────────────┘
                Total: ~2 seconds
```

**Speedup**: ~3x for 3 parallel calls, scales with more chunks!

## Usage Examples

### Example 1: Processing Multiple Chunks

```python
# In REPL code generated by the root LLM:
```repl
# Split context into chunks
chunks = [context[i:i+10000] for i in range(0, len(context), 10000)]

# Create prompts for each chunk
prompts = [f"Find any magic numbers in this chunk: {chunk}" for chunk in chunks]

# Process all chunks in parallel
results = llm_batch(prompts, max_concurrent=5)

# Combine results
all_findings = "\n".join(results)
print(f"Found: {all_findings}")
```
```

### Example 2: Parallel Summarization

```python
```repl
import re

# Split by sections
sections = re.split(r'### (.+)', context)

# Build prompts for all sections
prompts = []
headers = []
for i in range(1, len(sections), 2):
    header = sections[i]
    content = sections[i+1]
    prompts.append(f"Summarize this {header} section: {content}")
    headers.append(header)

# Summarize all sections in parallel
summaries = llm_batch(prompts)

# Combine results
buffers = [f"{h}: {s}" for h, s in zip(headers, summaries)]
final = llm_query(f"Combine these summaries: " + "\n".join(buffers))
```
```

### Example 3: Rate Limiting

```python
```repl
# If you have many prompts and want to avoid rate limits,
# use max_concurrent to control parallelism
prompts = [f"Process item {i}" for i in range(100)]

# Process in batches of 5 concurrent requests
results = llm_batch(prompts, max_concurrent=5)
```
```

## API Reference

### `llm_batch(prompts: List[str], max_concurrent: int = 10) -> List[str]`

**Parameters:**
- `prompts`: List of string prompts to send to the LLM
- `max_concurrent`: Maximum number of concurrent API requests (default: 10)

**Returns:**
- List of string responses in the same order as input prompts

**Error Handling:**
- If a single request fails, it returns an error message for that prompt
- Other successful requests are still returned
- The function always returns a list of the same length as input

## Best Practices

### 1. Use `llm_batch()` for Independent Queries
✅ **Good**: When queries don't depend on each other
```python
prompts = [f"Analyze document {i}" for i in range(10)]
results = llm_batch(prompts)
```

❌ **Bad**: When later queries need results from earlier ones
```python
# This won't work - each query depends on previous results
# Use sequential llm_query() instead
```

### 2. Adjust `max_concurrent` Based on Your Needs
- **Default (10)**: Good for most use cases
- **Lower (3-5)**: If hitting rate limits or want to be conservative
- **Higher (20+)**: If you have high rate limits and many chunks

### 3. Chunk Sizes
- Each sub-LLM can handle ~500K characters
- Don't make chunks too small (overhead of API calls)
- Don't make chunks too large (might hit token limits)
- Sweet spot: 10K-100K characters per chunk

### 4. Monitor Performance
The logger will show execution times for batch operations:
```
[REPL] Executing code... (completed in 2.34s)
```

## Running the Example

```bash
# Activate your conda environment
conda activate tf-gpu-211

# Run the batch example
python example_batch.py
```

## Technical Details

### Async Implementation
- Uses `asyncio.gather()` for concurrent execution
- Semaphore-based rate limiting to control concurrency
- Thread-safe execution from synchronous REPL code
- Handles event loop management automatically

### Error Handling
- Individual request failures don't crash the entire batch
- Exceptions are caught and returned as error strings
- Maintains order of results matching input prompts

### Compatibility
- Fully backward compatible with existing code
- `llm_query()` still works exactly as before
- No changes needed to existing RLM code

## Troubleshooting

### Rate Limit Errors
If you see rate limit errors:
```python
# Reduce max_concurrent
results = llm_batch(prompts, max_concurrent=3)
```

### Timeout Errors
If requests are timing out:
- Reduce chunk sizes
- Reduce `max_concurrent`
- Check network connectivity

### Memory Issues
If processing very large batches:
- Process in smaller batches
- Use `max_concurrent` to limit memory usage

## Future Enhancements

Possible future improvements:
- [ ] Exponential backoff retry logic
- [ ] Progress callbacks for long-running batches
- [ ] Cost tracking for batch operations
- [ ] Streaming responses for batch operations
- [ ] Full async RLM for end-to-end async processing

